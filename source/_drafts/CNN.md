---
title: CNN
category:
  - 笔记
date: 2019-01-10 20:44:45
tags:
---

# PPT

## 人工智能

Artificial Intelligence

### 定义

人工智能（英语：Artificial Intelligence，缩写为 AI）亦称机器智能，指由人制造出来的机器所表现出来的智能。
通过普通计算机程序的手段实现的人类智能技术。
系统正确解释外部数据，从这些数据中学习，并利用这些知识通过灵活适应实现特定目标和任务的能力。

蚁群算法、遗传算法、模拟退火算法

不需要特定编程


### 类别

* 强人工智能观点认为“有可能”制造出“真正”能推理（Reasoning）和解决问题的智能机器，并且，这样的机器将被认为是具有知觉、有自我意识的。
* 弱人工智能观点认为“不可能”制造出能“真正”地推理和解决问题的智能机器，这些机器只不过“看起来”像是智能的，但是并不真正拥有智能，也不会有自主意识。

## 机器学习

### 定义

* 模型
* 策略
* 算法

- Langley（1996) 定义的机器学习是“机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。
（Machine learning is a science of the artificial. The field's main objects of study are artifacts, specifically algorithms that improve their performance with experience.'）
- Tom Mitchell的机器学习(1997)对信息论中的一些概念有详细的解释,其中定义机器学习时提到，“机器学习是对能通过经验自动改进的计算机算法的研究”。
（Machine Learning is the study of computer algorithms that improve automatically through experience.）
- Alpaydin（2004）同时提出自己对机器学习的定义，“机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。”
（Machine learning is programming computers to optimize a performance criterion using example data or past experience.）
- 机器学习是一门研究机器获取新知识和新技能，并识别现有知识的学问。

1959年美国的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。

### 类别

* 监督学习(supervised learning)
监督学习，即在机械学习过程中提供对错指示。一般是在数据组中包含最终结果（0，1）。通过算法让机器自我减少误差。这一类学习主要应用于分类和预测 (regression & classify)。监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。
监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。
监督学习是从标记的训练数据来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。监督学习算法是分析该训练数据，并产生一个推断的功能，其可以用于映射出新的实例。一个最佳的方案将允许该算法来正确地决定那些看不见的实例的类标签。这就要求学习算法是在一种“合理”的方式从一种从训练数据到看不见的情况下形成。
* 非监督学习(unsupervised learning)
非监督学习又称归纳性学习（clustering）利用K方式(Kmeans)，建立中心（centriole），通过循环和递减运算(iteration&descent)来减小误差，达到分类的目的。
在机器学习，无监督学习的问题是，在未加标签的数据中，试图找到隐藏的结构。因为提供给学习者的实例是未标记的，因此没有错误或报酬信号来评估潜在的解决方案。这区别于监督学习和强化学习无监督学习。
无监督学习是密切相关的统计数据密度估计的问题。然而无监督学习还包括寻求，总结和解释数据的主要特点等诸多技术。在无监督学习使用的许多方法是基于用于处理数据的数据挖掘方法。
* 半监督学习
半监督学习(Semi-Supervised Learning，SSL)是模式识别和机器学习领域研究的重点问题，是监督学习与无监督学习相结合的一种学习方法。半监督学习使用大量的未标记数据，以及同时使用标记数据，来进行模式识别工作。当使用半监督学习时，将会要求尽量少的人员来从事工作，同时，又能够带来比较高的准确性
* 强化学习
强化学习是智能体（Agent）以“试错”的方式进行学习，通过与环境进行交互获得的奖赏指导行为，目标是使智能体获得最大的奖赏，强化学习不同于连接主义学习中的监督学习，主要表现在强化信号上，强化学习中由环境提供的强化信号是对产生动作的好坏作一种评价(通常为标量信号)，而不是告诉强化学习系统RLS(reinforcement learning system)如何去产生正确的动作。由于外部环境提供的信息很少，RLS必须靠自身的经历进行学习。通过这种方式，RLS在行动-评价的环境中获得知识，改进行动方案以适应环境。

#### 人工神经网络

>人工神经网络（Artificial Neural Network，即ANN ），是20世纪80 年代以来人工智能领域兴起的研究热点。它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。在工程与学术界也常直接简称为神经网络或类神经网络。神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成。每个节点代表一种特定的输出函数，称为激励函数（activation function）。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。网络的输出则依网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。
最近十多年来，人工神经网络的研究工作不断深入，已经取得了很大的进展，其在模式识别、智能机器人、自动控制、预测估计、生物、医学、经济等领域已成功地解决了许多现代计算机难以解决的实际问题，表现出了良好的智能特性。

https://baike.baidu.com/item/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/10658952?fr=aladdin

#### 深度学习

深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。

深度学习前身是人工神经网络(artificial neural network,ANN),基本特点就是试图模仿人脑的神经元之间传递和处理信息的模式.在机器学习中,我们说的神经网络一般就是指人工神经网络.
人工神经网络由各个层组成,输入层(input layer)输入训练数据,在输出层(output layer)输出计算结果,中间有一个或多个隐藏层(hidden layer),使输入数据向前传播到输出层.深度一般是㧈要求隐藏层很多,一般指5层,10层,几百层甚至几千层.

##### 网络结构

卷积神经网络：基本上就是用共享权重在空间中进行扩展的标准神经网络。设计CNN主要是为了通过内部卷积来识别图片，内部卷积可以看到待识别物体的边。

循环神经网络：基本上是在时间上进行扩展的标准神经网络，因为边进入下一个时间步，而不是在同一时间步进入下一个层。设计RNN主要是为了识别序列，例如语音信号或者文本。它里面的循环意味着网络中存在短暂的记忆。

递归神经网络：更类似于分层网络，其中输入序列没有真正的时间面，而是输入必须以树状方式分层处理。

##### 算法

https://blog.csdn.net/qq_34470213/article/details/79592484

反向传播

随机梯度下降

学习率衰减

dropout

max pooling

批标准化

LTSM long short-term memory

skip-gram

连续词袋

迁移学习

https://blog.csdn.net/NORTHhan/article/details/72724058

线性回归
逻辑回归
决策树
SVM
朴素贝叶斯
K最近邻算法
K均值算法
随机森林算法
降维算法
Gradient Boost 和 Adaboost 算法

机器学习的流派
连接主义：主要代表形式是人工神经网络

行为主义：代表形式是强化学习

符号主义：代表形式是专家系统和知识图谱

### 应用
https://blog.csdn.net/jackkang01/article/details/81064114


AlphaGo

### 示例

#### MNIST NN

#### Fast Style Transfer CNN

#### FlappyBird DQN

Deep Q Network
http://www.algorithmdog.com/drl


### 水表读数识别

#### 卷积神经网络

1、卷积神经网络CNN
从神经学角度来说，卷积神经网络的设计灵感来自人脑视觉皮层对外界事物的感知，人眼以图像的形式把感知到的事物传递给大脑，大脑童工逐层的对该图像进行抽象，抽取出图像的边角等代表图像的高维特征给大脑作出准确的判断。

卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构[2]。

CNN的两个核心操作：卷积和池化.

卷积：主要作用是抽取特征，是网络具有一定转移不变性，也有一定降维作用。一般设定一个3*3或5*5的卷积窗口，采用relu激活函数，对输入X进行卷积操作。卷积可能是单通道也可能是多通道的。操作时分为padding和非padding两种方式。padding分zero-或mean-等。对同一个输入可以设置不同的卷积窗口或步长来尽可能的多抽取特征。
池化：主要起到降维作用。设置一个池化窗口，对X进行池化，采用relu或sigmod做激活函数，注意函数的饱和死区特性导致的反向传播时梯度消失问题，可以配合Batch Normalization使用。池化有最大池化或平均池化。
CNN的三个概念：局部感知野、权值共享和下采样/降采样.

局部感知野：卷积操作时卷积与X重合的部分。
权值共享：卷积或池化操作时，窗口的权值保持不变。
下采样：即池化操作

#### 物体分割

优点
* 分割出读数区域,减少复杂度
* 提高读数区域的分辨率,提高识别质量
* 技术可用于条形码/二维码/钢印号的识别

##### cnn 分类原理

##### MNIST CNN

http://scs.ryerson.ca/~aharley/vis/conv/flat.html

##### 分类模型物体分割原理

##### 常用网络

GoogleNet
AlexNet
embedded
ssd
mobilenet
resnet
fpn
faster_rcnn
inception
ImageNet
VGG

##### 常用数据集

>MNIST<br>
深度学习领域的“Hello World!”，入门必备!MNIST是一个手写数字数据库，它有60000个训练样本集和10000个测试样本集，每个样本图像的宽高为28*28。此数据集是以二进制存储的，不能直接以图像格式查看，不过很容易找到将其转换成图像格式的工具。
　　最早的深度卷积网络LeNet便是针对此数据集的，当前主流深度学习框架几乎无一例外将MNIST数据集的处理作为介绍及入门第一教程，其中Tensorflow关于MNIST的教程非常详细。


>MSCOCO<br>
>COCO(Common Objects in Context)是一个新的图像识别、分割、和字幕数据集，它有如下特点：<br>
>- 对象分割
>- 上下文识别
>- 每个图像的多个对象
>- 超过300000幅图像
>- 超过200万个实例
>- 80个对象类别
>- 每个图像5个说明文字
>- 100000人的关键点<br><br>
>COCO数据集由微软赞助，其对于图像的标注信息不仅有类别、位置信息，还有对图像的语义文本描述，COCO数据集的开源使得近两三年来图像分割语义理解取得了巨大的进展，也几乎成为了图像语义理解算法性能评价的“标准”数据集。

>OpenImage<br>
包含在 190 万张图片上针对 600 个类别的 1540 万个边框盒，这也是现有最大的具有对象位置注释的数据集。这些边框盒大部分都是由专业注释人员手动绘制的，确保了它们的准确性和一致性。另外，这些图像是非常多样化的，并且通常包含有多个对象的复杂场景（平均每个图像 8 个）。

>ImageNet
ImageNet数据集有1400多万幅图片，涵盖2万多个类别。其中有超过百万的图片有明确的类别标注和图像中物体位置的标注，相关信息如下：
>* 非空的同义词集总数：21841 
>* 图像总数：14,197,122
>* 边界框注释的图像数：1,034,908
>* 具有SIFT特征的同义词集数：1000
>* 具有SIFT特征的图像数：120万<br><br>
Imagenet数据集是目前深度学习图像领域应用得非常多的一个领域，关于图像分类、定位、检测等研究工作大多基于此数据集展开。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。数据集下载~1TB(ILSVRC2016比赛全部数据)
ImageNet是根据WordNet层次（目前只有名词）组织的一个图像数据库，其中每个节点的层次结构是由成千上万的图像描绘。目前，有平均每个节点超过五百的图像。我们希望对于研究人员，教育工作者，学生和所有分享我们对图片激情的人来说，ImageNet能成为一个有用的资源。
点击这里了解更多关于ImageNet，请点击这里加入ImageNet邮件列表。


>PASCAL VOC<br>
PASCAL VOC挑战赛是视觉对象的分类识别和检测的一个基准测试，提供了检测算法和学习性能的标准图像注释数据集和标准的评估系统。PASCAL VOC图片集包括20个目录：人类;动物(鸟、猫、牛、狗、马、羊);交通工具(飞机、自行车、船、公共汽车、小轿车、摩托车、火车);室内(瓶子、椅子、餐桌、盆栽植物、沙发、电视)。PASCAL VOC挑战赛在2012年后便不再举办，但其数据集图像质量好，标注完备，非常适合用来测试算法性能。

#### 读数识别(OCR)

优点
* 性能好
* 标注成本低

http://scs.ryerson.ca/~aharley/vis/conv/flat.html


监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。
监督学习和非监督学习的差别就是训练集目标是否人标注。他们都有训练集 且都有输入和输出
无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有生成对抗网络（GAN）、聚类。
半监督学习介于监督学习与无监督学习之间。
增强学习机器为了达成目标，随着环境的变动，而逐步调整其行为，并评估每一个行动之后所到的回馈是正向的或负向的。



结构（Architecture）结构指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量可以是神经元连接的权重（weights）和神经元的激励值（activities of the neurons）。
激励函数（Activity Rule）大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激励函数依赖于网络中的权重（即该网络的参数）。
学习规则（Learning Rule）学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间尺度的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的目标值和当前权重的值。例如，用于手写识别的一个神经网络，有一组输入神经元。输入神经元会被输入图像的数据所激发。在激励值被加权并通过一个函数（由网络的设计者确定）后，这些神经元的激励值被传递到其他神经元。这个过程不断重复，直到输出神经元被激发。最后，输出神经元的激励值决定了识别出来的是哪个字母。

LeNet，这是最早用于数字识别的CNN
AlexNet， 2012 ILSVRC比赛远超第2名的CNN，比
LeNet更深，用多层小卷积层叠加替换单大卷积层。
ZF Net， 2013 ILSVRC比赛冠军
GoogLeNet， 2014 ILSVRC比赛冠军
VGGNet， 2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好